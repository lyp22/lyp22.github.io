---
published: true
title: 概率图模型：HMM、MEMM、CRF
category: Algorithm
tags: 
  - 概率图
layout: post
---



概率图模型学习笔记：HMM、MEMM、CRF

# 一、Preface
> 统计机器学习所有的模型（个别instant model和优化算法以及其他的特种工程知识点除外）的工作流程都是如此：

> a.训练模型参数，得到模型（由参数唯一确定），

> b.预测给定的测试数据。

> 拿这个流程去挨个学习模型，思路上会非常顺畅。
 
# 二、Prerequisite
## 2.1 概率图
之前刚接触CRF时，一上来试图越过一堆繁琐的概率图相关概念，不过sad to say, 这是后面的前驱知识，后面还得反过来补这个点。所以若想整体把握，系统地拿下这一块，应该还是要越过这块门槛的。 当然了，一开始只需略略快速看一篇，后面可再返过来补查。
### 2.1.1 概览
在统计概率图（probability graph models）中，参考宗成庆老师的书，是这样的体系结构（个人非常喜欢这种类型的图）：

![0](https://raw.githubusercontent.com/lyp22/lyp22.github.io/master/_posts/image/CRF/1.jpg)

在概率图模型中，数据(样本)由公式G=(V,E)建模表示：  V表示节点，即随机变量（放在此处的，可以是一个token或者一个label），具体地，用Y=(y1,y2,……,yn)  为随机变量建模，注意  现在是代表了一批随机变量（想象对应一条sequence，包含了很多的token），  P(Y)为这些随机变量的分布； E表示边，即概率依赖关系。具体咋理解，还是要在后面结合HMM或CRF的graph具体解释。


### 2.1.2 有向图 vs. 无向图
上图可以看到，贝叶斯网络（信念网络）都是有向的，马尔科夫网络无向。所以，贝叶斯网络适合为有单向依赖的数据建模，马尔科夫网络适合实体之间互相依赖的建模。具体地，他们的核心差异表现在如何求 P=(Y) ，即怎么表示 Y=(y1,y2,……,yn) 这个的联合概率。
#### 1. 有向图
对于有向图模型，这么求联合概率： 
![0](https://raw.githubusercontent.com/lyp22/lyp22.github.io/master/_posts/image/CRF/f1.jpg)


举个例子，对于下面的这个有向图的随机变量(注意，这个图我画的还是比较广义的)：

![0](https://raw.githubusercontent.com/lyp22/lyp22.github.io/master/_posts/image/CRF/2.jpg)

应该这样表示他们的联合概率:

![0](https://raw.githubusercontent.com/lyp22/lyp22.github.io/master/_posts/image/CRF/f2.jpg)

#### 2. 无向图

对于无向图，我看资料一般就指马尔科夫网络(注意，这个图我画的也是比较广义的)。

![0](https://raw.githubusercontent.com/lyp22/lyp22.github.io/master/_posts/image/CRF/3.jpg)

如果一个graph太大，可以用因子分解将 P=(Y) 写为若干个联合概率的乘积。咋分解呢，将一个图分为若干个“小团”，注意每个团必须是“最大团”（就是里面任何两个点连在了一块，具体……算了不解释，有点“最大连通子图”的感觉），则有：

![0](https://raw.githubusercontent.com/lyp22/lyp22.github.io/master/_posts/image/CRF/f3.jpg)

, 其中 ![0](https://raw.githubusercontent.com/lyp22/lyp22.github.io/master/_posts/image/CRF/f4.jpg) ，公式应该不难理解吧，归一化是为了让结果算作概率。

所以像上面的无向图：

![0](https://raw.githubusercontent.com/lyp22/lyp22.github.io/master/_posts/image/CRF/f5.jpg)

其中，  ![0](https://raw.githubusercontent.com/lyp22/lyp22.github.io/master/_posts/image/CRF/f6.jpg)是一个最大团 C 上随机变量们的联合概率，一般取指数函数的：

![0](https://raw.githubusercontent.com/lyp22/lyp22.github.io/master/_posts/image/CRF/f7.jpg)

好了，管这个东西叫做势函数。注意 ![0](https://raw.githubusercontent.com/lyp22/lyp22.github.io/master/_posts/image/CRF/f8.png) 是否有看到CRF的影子。

那么概率无向图的联合概率分布可以在因子分解下表示为：

![0](https://raw.githubusercontent.com/lyp22/lyp22.github.io/master/_posts/image/CRF/f9.png)

注意，这里的理解还蛮重要的，注意递推过程，敲黑板，这是CRF的开端！
这个由Hammersly-Clifford law保证，具体不展开。

### 2.1.3 马尔科夫假设&马尔科夫性

这个也属于前馈知识。

#### 1. 马尔科夫假设

额应该是齐次马尔科夫假设，这样假设：马尔科夫链(X1,……,Xn)里的Xi  总是只受Xi-1  一个人的影响。马尔科夫假设这里相当于就是个1-gram。马尔科夫过程呢？即，在一个过程中，每个状态的转移只依赖于前n个状态，并且只是个n阶的模型。最简单的马尔科夫过程是一阶的，即只依赖于器哪一个状态。

#### 2. 马尔科夫性

马尔科夫性是是保证或者判断概率图是否为概率无向图的条件。三点内容：a. 成对，b. 局部，c. 全局。我觉得这个不用展开。

## 2.2 判别式（discriminative）模型 vs. 生成式(generative)模型

在监督学习下，模型可以分为判别式模型与生成式模型。重点来了。上面有提到，我理解了HMM、CRF模型的区别是从理解了判别式模型与生成式模型的那刻，并且瞬间对其他的模型有一个恍然大悟。我记得是一年前就开始纠结这两者的区别，但我只能说，栽在了一些烂博客上，大部分都没有自己的insightful理解，也就是一顿官话，也真是难以理解。后来在知乎上一直琢磨别人的答案，然后某日早晨终于豁然开朗，就是这种感觉。好了，我要用自己的理解来转述两者的区别了below。先问个问题，根据经验，A批模型（神经网络模型、SVM、perceptron、LR、DT……）与B批模型（NB、LDA……），有啥区别不？（这个问题需要一些模型使用经验）应该是这样的：

> 1. A批模型是这么工作的，他们直接将数据的Y（或者label），根据所提供的features，学习，最后画出了一个明显或者比较明显的边界（具体怎么做到的？通过复杂的函数映射，或者决策叠加等等mechanism），这一点线性LR、线性SVM应该很明显吧。 

> 2. B批模型是这么工作的，他们先从训练样本数据中，将所有的数据的分布情况摸透，然后最终确定一个分布，来作为我的所有的输入数据的分布，并且他是一个联合分布P(X,Y)  (注意 X 包含所有的特征xi  ，  Y包含所有的label)。然后我来了新的样本数据（inference），好，通过学习来的模型的联合分布P(X,Y)  ，再结合新样本给的X  ，通过条件概率就能出来Y  ：
P(Y|X)=P(X,Y)/P(X)

好了，应该说清楚了。
